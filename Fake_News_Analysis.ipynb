{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Wook22/Fake_News_Classification/blob/main/Fake_News_Analysis.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "Kn2j1jVm6dfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Analysis on Fake News**\n",
        "\n",
        "### Abstract\n"
      ],
      "metadata": {
        "id": "IWep6-Vs6I2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Have you ever questioned whether the news you see is real or not? Since the internet became widespread, fake news has increasingly been used as a tool to manipulate public opinion. One of the most well-known examples is the Nayirah testimony. On October 10, 1990, a 15-year-old Kuwaiti girl gave a false testimony before the United States Congressional Human Rights Caucus. She claimed to be a volunteer nurse at a Kuwaiti hospital during the Iraqi invasion. In her testimony, she said she witnessed Iraqi soldiers removing premature babies from incubators, stealing the equipment, and leaving the babies to die on the floor. This emotional account played a significant role in shaping public support and helped President George H. W. Bush justify military action against Iraq.\n",
        "\n",
        "However,\n",
        "\n",
        "\"was shown to be almost certainly false by an ABC reporter, John Martin, in March 1991\" (The New York Times)\n",
        "\n",
        "In January 1992, it was revealed that she had never been a nurse and was, in fact, the daughter of Saud Nasser Al-Saud Al-Sabah, the Kuwaiti ambassador to the United States at the time of her testimony. This raises an important question: What should we believe, and what should we not? In an age where misinformation can spread quickly, it's becoming increasingly difficult to know what is true and what is not.\n",
        "\n",
        "Throughout this project, I will develop a model that predicts whether a news article is real or fake based on the count of phrases and language used in the text.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Pws6kWRX6z4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Description\n",
        "\n"
      ],
      "metadata": {
        "id": "5RHgr0r8Nf5v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n10NWW683AfL"
      },
      "outputs": [],
      "source": [
        "df_real = read.csv(\"BuzzFeed_real_news_content.csv\")\n",
        "df_fake = read.csv(\"BuzzFeed_fake_news_content.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colnames(df_real)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "VPXDt0igVDCa",
        "outputId": "3c52a307-5c7c-46b1-e9ec-4db4a0fc98c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'id'</li><li>'title'</li><li>'text'</li><li>'url'</li><li>'top_img'</li><li>'authors'</li><li>'source'</li><li>'publish_date'</li><li>'movies'</li><li>'images'</li><li>'canonical_link'</li><li>'meta_data'</li></ol>\n"
            ],
            "text/markdown": "1. 'id'\n2. 'title'\n3. 'text'\n4. 'url'\n5. 'top_img'\n6. 'authors'\n7. 'source'\n8. 'publish_date'\n9. 'movies'\n10. 'images'\n11. 'canonical_link'\n12. 'meta_data'\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 'id'\n\\item 'title'\n\\item 'text'\n\\item 'url'\n\\item 'top\\_img'\n\\item 'authors'\n\\item 'source'\n\\item 'publish\\_date'\n\\item 'movies'\n\\item 'images'\n\\item 'canonical\\_link'\n\\item 'meta\\_data'\n\\end{enumerate*}\n",
            "text/plain": [
              " [1] \"id\"             \"title\"          \"text\"           \"url\"           \n",
              " [5] \"top_img\"        \"authors\"        \"source\"         \"publish_date\"  \n",
              " [9] \"movies\"         \"images\"         \"canonical_link\" \"meta_data\"     "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colnames(df_fake)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "1AcFMTCgVaDI",
        "outputId": "e715e66d-c148-499a-d0fc-947fd04d95c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'id'</li><li>'title'</li><li>'text'</li><li>'url'</li><li>'top_img'</li><li>'authors'</li><li>'source'</li><li>'publish_date'</li><li>'movies'</li><li>'images'</li><li>'canonical_link'</li><li>'meta_data'</li></ol>\n"
            ],
            "text/markdown": "1. 'id'\n2. 'title'\n3. 'text'\n4. 'url'\n5. 'top_img'\n6. 'authors'\n7. 'source'\n8. 'publish_date'\n9. 'movies'\n10. 'images'\n11. 'canonical_link'\n12. 'meta_data'\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 'id'\n\\item 'title'\n\\item 'text'\n\\item 'url'\n\\item 'top\\_img'\n\\item 'authors'\n\\item 'source'\n\\item 'publish\\_date'\n\\item 'movies'\n\\item 'images'\n\\item 'canonical\\_link'\n\\item 'meta\\_data'\n\\end{enumerate*}\n",
            "text/plain": [
              " [1] \"id\"             \"title\"          \"text\"           \"url\"           \n",
              " [5] \"top_img\"        \"authors\"        \"source\"         \"publish_date\"  \n",
              " [9] \"movies\"         \"images\"         \"canonical_link\" \"meta_data\"     "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_real[\"real_fake\"] = 0\n",
        "df_fake[\"real_fake\"] = 1\n",
        "\n",
        "buzzfeed = rbind(df_real, df_fake)"
      ],
      "metadata": {
        "id": "5rxfcOs2Vm-7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(c(\"tidytext\", \"dplyr\", \"stringr\", \"tidyr\"))\n",
        "\n",
        "# Load necessary libraries\n",
        "library(dplyr)\n",
        "library(stringr)\n",
        "library(tidytext)\n",
        "library(tidyr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jcd7tl14WIoJ",
        "outputId": "565f06b2-2e9c-432f-f63a-07a41014e733"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘SnowballC’, ‘janeaustenr’, ‘tokenizers’\n",
            "\n",
            "\n",
            "\n",
            "Attaching package: ‘dplyr’\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:stats’:\n",
            "\n",
            "    filter, lag\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:base’:\n",
            "\n",
            "    intersect, setdiff, setequal, union\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary dataset\n",
        "df <- buzzfeed\n",
        "\n",
        "# Tokenize text and count word frequencies\n",
        "all_words <- df %>%\n",
        "  select(text) %>%\n",
        "  unnest_tokens(word, text) %>%\n",
        "  count(word, sort = TRUE)\n",
        "\n",
        "# Function to estimate syllables by counting vowel groups\n",
        "estimate_syllables <- function(word) {\n",
        "  str_count(tolower(word), \"[aeiouy]+\")\n",
        "}\n",
        "\n",
        "# Add syllable counts\n",
        "all_words <- all_words %>%\n",
        "  mutate(syllables = estimate_syllables(word)) %>%\n",
        "  filter(syllables > 2)\n",
        "\n",
        "# Get top 50 words with more than 3 syllables\n",
        "top50_words <- head(all_words$word, 50)\n",
        "\n",
        "# Function to count word occurrences in text\n",
        "count_word <- function(text, word) {\n",
        "  str_count(tolower(text), fixed(tolower(word)))\n",
        "}\n",
        "\n",
        "# Create new columns for each top word\n",
        "for (w in top50_words) {\n",
        "  df[[paste0(\"word_\", w)]] <- sapply(df$text, count_word, word = w)\n",
        "}"
      ],
      "metadata": {
        "id": "eLGPEITtYqtb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'id''title''text''url''top_img''authors''source'\n",
        "# 'publish_date''movies''images''canonical_link''meta_data'\n",
        "\n",
        "# Remove columns by name\n",
        "df_drop <- df %>%\n",
        "  select(-title, -source, -id, -text, -url, -top_img, -movies, -images, -canonical_link, -meta_data)"
      ],
      "metadata": {
        "id": "vvtNQJA-Zx9d"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colnames(df_drop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "TeVTc-lbvd1s",
        "outputId": "df49f917-aaca-4e85-962b-f8640565cb6d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'authors'</li><li>'publish_date'</li><li>'real_fake'</li><li>'word_hillary'</li><li>'word_president'</li><li>'word_debate'</li><li>'word_obama'</li><li>'word_police'</li><li>'word_because'</li><li>'word_american'</li><li>'word_presidential'</li><li>'word_before'</li><li>'word_america'</li><li>'word_election'</li><li>'word_republican'</li><li>'word_foundation'</li><li>'word_according'</li><li>'word_every'</li><li>'word_september'</li><li>'word_united'</li><li>'word_another'</li><li>'word_political'</li><li>'word_candidate'</li><li>'word_americans'</li><li>'word_national'</li><li>'word_charlotte'</li><li>'word_democratic'</li><li>'word_policy'</li><li>'word_something'</li><li>'word_washington'</li><li>'word_nominee'</li><li>'word_security'</li><li>'word_federal'</li><li>'word_terrorism'</li><li>'word_department'</li><li>'word_government'</li><li>'word_actually'</li><li>'word_rahami'</li><li>'word_continued'</li><li>'word_everyone'</li><li>'word_general'</li><li>'word_office'</li><li>'word_terrorists'</li><li>'word_business'</li><li>'word_islamic'</li><li>'word_statement'</li><li>'word_anyone'</li><li>'word_debates'</li><li>'word_candidates'</li><li>'word_military'</li><li>'word_including'</li><li>'word_officials'</li><li>'word_terrorist'</li></ol>\n"
            ],
            "text/markdown": "1. 'authors'\n2. 'publish_date'\n3. 'real_fake'\n4. 'word_hillary'\n5. 'word_president'\n6. 'word_debate'\n7. 'word_obama'\n8. 'word_police'\n9. 'word_because'\n10. 'word_american'\n11. 'word_presidential'\n12. 'word_before'\n13. 'word_america'\n14. 'word_election'\n15. 'word_republican'\n16. 'word_foundation'\n17. 'word_according'\n18. 'word_every'\n19. 'word_september'\n20. 'word_united'\n21. 'word_another'\n22. 'word_political'\n23. 'word_candidate'\n24. 'word_americans'\n25. 'word_national'\n26. 'word_charlotte'\n27. 'word_democratic'\n28. 'word_policy'\n29. 'word_something'\n30. 'word_washington'\n31. 'word_nominee'\n32. 'word_security'\n33. 'word_federal'\n34. 'word_terrorism'\n35. 'word_department'\n36. 'word_government'\n37. 'word_actually'\n38. 'word_rahami'\n39. 'word_continued'\n40. 'word_everyone'\n41. 'word_general'\n42. 'word_office'\n43. 'word_terrorists'\n44. 'word_business'\n45. 'word_islamic'\n46. 'word_statement'\n47. 'word_anyone'\n48. 'word_debates'\n49. 'word_candidates'\n50. 'word_military'\n51. 'word_including'\n52. 'word_officials'\n53. 'word_terrorist'\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 'authors'\n\\item 'publish\\_date'\n\\item 'real\\_fake'\n\\item 'word\\_hillary'\n\\item 'word\\_president'\n\\item 'word\\_debate'\n\\item 'word\\_obama'\n\\item 'word\\_police'\n\\item 'word\\_because'\n\\item 'word\\_american'\n\\item 'word\\_presidential'\n\\item 'word\\_before'\n\\item 'word\\_america'\n\\item 'word\\_election'\n\\item 'word\\_republican'\n\\item 'word\\_foundation'\n\\item 'word\\_according'\n\\item 'word\\_every'\n\\item 'word\\_september'\n\\item 'word\\_united'\n\\item 'word\\_another'\n\\item 'word\\_political'\n\\item 'word\\_candidate'\n\\item 'word\\_americans'\n\\item 'word\\_national'\n\\item 'word\\_charlotte'\n\\item 'word\\_democratic'\n\\item 'word\\_policy'\n\\item 'word\\_something'\n\\item 'word\\_washington'\n\\item 'word\\_nominee'\n\\item 'word\\_security'\n\\item 'word\\_federal'\n\\item 'word\\_terrorism'\n\\item 'word\\_department'\n\\item 'word\\_government'\n\\item 'word\\_actually'\n\\item 'word\\_rahami'\n\\item 'word\\_continued'\n\\item 'word\\_everyone'\n\\item 'word\\_general'\n\\item 'word\\_office'\n\\item 'word\\_terrorists'\n\\item 'word\\_business'\n\\item 'word\\_islamic'\n\\item 'word\\_statement'\n\\item 'word\\_anyone'\n\\item 'word\\_debates'\n\\item 'word\\_candidates'\n\\item 'word\\_military'\n\\item 'word\\_including'\n\\item 'word\\_officials'\n\\item 'word\\_terrorist'\n\\end{enumerate*}\n",
            "text/plain": [
              " [1] \"authors\"           \"publish_date\"      \"real_fake\"        \n",
              " [4] \"word_hillary\"      \"word_president\"    \"word_debate\"      \n",
              " [7] \"word_obama\"        \"word_police\"       \"word_because\"     \n",
              "[10] \"word_american\"     \"word_presidential\" \"word_before\"      \n",
              "[13] \"word_america\"      \"word_election\"     \"word_republican\"  \n",
              "[16] \"word_foundation\"   \"word_according\"    \"word_every\"       \n",
              "[19] \"word_september\"    \"word_united\"       \"word_another\"     \n",
              "[22] \"word_political\"    \"word_candidate\"    \"word_americans\"   \n",
              "[25] \"word_national\"     \"word_charlotte\"    \"word_democratic\"  \n",
              "[28] \"word_policy\"       \"word_something\"    \"word_washington\"  \n",
              "[31] \"word_nominee\"      \"word_security\"     \"word_federal\"     \n",
              "[34] \"word_terrorism\"    \"word_department\"   \"word_government\"  \n",
              "[37] \"word_actually\"     \"word_rahami\"       \"word_continued\"   \n",
              "[40] \"word_everyone\"     \"word_general\"      \"word_office\"      \n",
              "[43] \"word_terrorists\"   \"word_business\"     \"word_islamic\"     \n",
              "[46] \"word_statement\"    \"word_anyone\"       \"word_debates\"     \n",
              "[49] \"word_candidates\"   \"word_military\"     \"word_including\"   \n",
              "[52] \"word_officials\"    \"word_terrorist\"   "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_time_adjust <- df_drop %>%\n",
        "  mutate(\n",
        "    publish_timestamp = as.numeric(str_extract(publish_date, \"\\\\d+\")),\n",
        "    publish_date = as.Date(as.POSIXct(publish_timestamp / 1000, origin = \"1970-01-01\", tz = \"UTC\"))\n",
        "  ) %>%\n",
        "  select(-publish_timestamp)"
      ],
      "metadata": {
        "id": "WBrDJPyYv4k2"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(colSums(is.na(df_time_adjust)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogVNF4TDwN1N",
        "outputId": "b2e08500-2366-4d5d-f3d0-0f855f84e246"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          authors      publish_date         real_fake      word_hillary \n",
            "                0                49                 0                 0 \n",
            "   word_president       word_debate        word_obama       word_police \n",
            "                0                 0                 0                 0 \n",
            "     word_because     word_american word_presidential       word_before \n",
            "                0                 0                 0                 0 \n",
            "     word_america     word_election   word_republican   word_foundation \n",
            "                0                 0                 0                 0 \n",
            "   word_according        word_every    word_september       word_united \n",
            "                0                 0                 0                 0 \n",
            "     word_another    word_political    word_candidate    word_americans \n",
            "                0                 0                 0                 0 \n",
            "    word_national    word_charlotte   word_democratic       word_policy \n",
            "                0                 0                 0                 0 \n",
            "   word_something   word_washington      word_nominee     word_security \n",
            "                0                 0                 0                 0 \n",
            "     word_federal    word_terrorism   word_department   word_government \n",
            "                0                 0                 0                 0 \n",
            "    word_actually       word_rahami    word_continued     word_everyone \n",
            "                0                 0                 0                 0 \n",
            "     word_general       word_office   word_terrorists     word_business \n",
            "                0                 0                 0                 0 \n",
            "     word_islamic    word_statement       word_anyone      word_debates \n",
            "                0                 0                 0                 0 \n",
            "  word_candidates     word_military    word_including    word_officials \n",
            "                0                 0                 0                 0 \n",
            "   word_terrorist \n",
            "                0 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dropna <- df_time_adjust %>% drop_na(publish_date)\n",
        "\n",
        "buzzfeed_data <- df_dropna\n",
        "head(buzzfeed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "Dq-GblBYwhtN",
        "outputId": "4de4b554-b6e6-42f8-ce52-b501e5ffe442"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 6 × 53</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>authors</th><th scope=col>publish_date</th><th scope=col>real_fake</th><th scope=col>word_hillary</th><th scope=col>word_president</th><th scope=col>word_debate</th><th scope=col>word_obama</th><th scope=col>word_police</th><th scope=col>word_because</th><th scope=col>word_american</th><th scope=col>⋯</th><th scope=col>word_business</th><th scope=col>word_islamic</th><th scope=col>word_statement</th><th scope=col>word_anyone</th><th scope=col>word_debates</th><th scope=col>word_candidates</th><th scope=col>word_military</th><th scope=col>word_including</th><th scope=col>word_officials</th><th scope=col>word_terrorist</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;date&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>⋯</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td>View All Posts,Leonora Cravotta                                 </td><td>2016-09-22</td><td>0</td><td>1</td><td>1</td><td> 0</td><td>0</td><td>2</td><td>0</td><td>0</td><td>⋯</td><td>0</td><td>0</td><td>0</td><td>1</td><td> 0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>4</td></tr>\n",
              "\t<tr><th scope=row>2</th><td>Cassy Fiano                                                     </td><td>2016-09-21</td><td>0</td><td>0</td><td>0</td><td> 0</td><td>9</td><td>0</td><td>1</td><td>1</td><td>⋯</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td></tr>\n",
              "\t<tr><th scope=row>3</th><td>Jack Shafer,Erick Trickey,Zachary Karabell                      </td><td>2016-09-27</td><td>0</td><td>3</td><td>1</td><td> 1</td><td>1</td><td>0</td><td>0</td><td>5</td><td>⋯</td><td>1</td><td>0</td><td>0</td><td>0</td><td> 0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>4</th><td>Cassy Fiano                                                     </td><td>2016-09-21</td><td>0</td><td>1</td><td>0</td><td> 0</td><td>0</td><td>4</td><td>0</td><td>1</td><td>⋯</td><td>0</td><td>0</td><td>1</td><td>1</td><td> 0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>5</th><td>Jack Shafer,Steven Shepard,Glenn Thrush,Nolan D,Shane Goldmacher</td><td>2016-09-26</td><td>0</td><td>9</td><td>6</td><td>19</td><td>1</td><td>0</td><td>4</td><td>0</td><td>⋯</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 6</td><td>5</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>\n",
              "\t<tr><th scope=row>6</th><td>Jack Shafer,Jeff Greenfield                                     </td><td>2016-09-26</td><td>0</td><td>2</td><td>4</td><td>24</td><td>6</td><td>0</td><td>2</td><td>1</td><td>⋯</td><td>2</td><td>0</td><td>0</td><td>0</td><td>11</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 6 × 53\n\n| <!--/--> | authors &lt;chr&gt; | publish_date &lt;date&gt; | real_fake &lt;dbl&gt; | word_hillary &lt;int&gt; | word_president &lt;int&gt; | word_debate &lt;int&gt; | word_obama &lt;int&gt; | word_police &lt;int&gt; | word_because &lt;int&gt; | word_american &lt;int&gt; | ⋯ ⋯ | word_business &lt;int&gt; | word_islamic &lt;int&gt; | word_statement &lt;int&gt; | word_anyone &lt;int&gt; | word_debates &lt;int&gt; | word_candidates &lt;int&gt; | word_military &lt;int&gt; | word_including &lt;int&gt; | word_officials &lt;int&gt; | word_terrorist &lt;int&gt; |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 1 | View All Posts,Leonora Cravotta                                  | 2016-09-22 | 0 | 1 | 1 |  0 | 0 | 2 | 0 | 0 | ⋯ | 0 | 0 | 0 | 1 |  0 | 1 | 0 | 0 | 1 | 4 |\n| 2 | Cassy Fiano                                                      | 2016-09-21 | 0 | 0 | 0 |  0 | 9 | 0 | 1 | 1 | ⋯ | 0 | 0 | 0 | 0 |  0 | 0 | 0 | 0 | 0 | 1 |\n| 3 | Jack Shafer,Erick Trickey,Zachary Karabell                       | 2016-09-27 | 0 | 3 | 1 |  1 | 1 | 0 | 0 | 5 | ⋯ | 1 | 0 | 0 | 0 |  0 | 0 | 0 | 0 | 0 | 0 |\n| 4 | Cassy Fiano                                                      | 2016-09-21 | 0 | 1 | 0 |  0 | 0 | 4 | 0 | 1 | ⋯ | 0 | 0 | 1 | 1 |  0 | 0 | 0 | 0 | 0 | 0 |\n| 5 | Jack Shafer,Steven Shepard,Glenn Thrush,Nolan D,Shane Goldmacher | 2016-09-26 | 0 | 9 | 6 | 19 | 1 | 0 | 4 | 0 | ⋯ | 0 | 0 | 0 | 0 |  6 | 5 | 0 | 1 | 0 | 0 |\n| 6 | Jack Shafer,Jeff Greenfield                                      | 2016-09-26 | 0 | 2 | 4 | 24 | 6 | 0 | 2 | 1 | ⋯ | 2 | 0 | 0 | 0 | 11 | 1 | 0 | 1 | 0 | 0 |\n\n",
            "text/latex": "A data.frame: 6 × 53\n\\begin{tabular}{r|lllllllllllllllllllll}\n  & authors & publish\\_date & real\\_fake & word\\_hillary & word\\_president & word\\_debate & word\\_obama & word\\_police & word\\_because & word\\_american & ⋯ & word\\_business & word\\_islamic & word\\_statement & word\\_anyone & word\\_debates & word\\_candidates & word\\_military & word\\_including & word\\_officials & word\\_terrorist\\\\\n  & <chr> & <date> & <dbl> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & ⋯ & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int> & <int>\\\\\n\\hline\n\t1 & View All Posts,Leonora Cravotta                                  & 2016-09-22 & 0 & 1 & 1 &  0 & 0 & 2 & 0 & 0 & ⋯ & 0 & 0 & 0 & 1 &  0 & 1 & 0 & 0 & 1 & 4\\\\\n\t2 & Cassy Fiano                                                      & 2016-09-21 & 0 & 0 & 0 &  0 & 9 & 0 & 1 & 1 & ⋯ & 0 & 0 & 0 & 0 &  0 & 0 & 0 & 0 & 0 & 1\\\\\n\t3 & Jack Shafer,Erick Trickey,Zachary Karabell                       & 2016-09-27 & 0 & 3 & 1 &  1 & 1 & 0 & 0 & 5 & ⋯ & 1 & 0 & 0 & 0 &  0 & 0 & 0 & 0 & 0 & 0\\\\\n\t4 & Cassy Fiano                                                      & 2016-09-21 & 0 & 1 & 0 &  0 & 0 & 4 & 0 & 1 & ⋯ & 0 & 0 & 1 & 1 &  0 & 0 & 0 & 0 & 0 & 0\\\\\n\t5 & Jack Shafer,Steven Shepard,Glenn Thrush,Nolan D,Shane Goldmacher & 2016-09-26 & 0 & 9 & 6 & 19 & 1 & 0 & 4 & 0 & ⋯ & 0 & 0 & 0 & 0 &  6 & 5 & 0 & 1 & 0 & 0\\\\\n\t6 & Jack Shafer,Jeff Greenfield                                      & 2016-09-26 & 0 & 2 & 4 & 24 & 6 & 0 & 2 & 1 & ⋯ & 2 & 0 & 0 & 0 & 11 & 1 & 0 & 1 & 0 & 0\\\\\n\\end{tabular}\n",
            "text/plain": [
              "  authors                                                          publish_date\n",
              "1 View All Posts,Leonora Cravotta                                  2016-09-22  \n",
              "2 Cassy Fiano                                                      2016-09-21  \n",
              "3 Jack Shafer,Erick Trickey,Zachary Karabell                       2016-09-27  \n",
              "4 Cassy Fiano                                                      2016-09-21  \n",
              "5 Jack Shafer,Steven Shepard,Glenn Thrush,Nolan D,Shane Goldmacher 2016-09-26  \n",
              "6 Jack Shafer,Jeff Greenfield                                      2016-09-26  \n",
              "  real_fake word_hillary word_president word_debate word_obama word_police\n",
              "1 0         1            1               0          0          2          \n",
              "2 0         0            0               0          9          0          \n",
              "3 0         3            1               1          1          0          \n",
              "4 0         1            0               0          0          4          \n",
              "5 0         9            6              19          1          0          \n",
              "6 0         2            4              24          6          0          \n",
              "  word_because word_american ⋯ word_business word_islamic word_statement\n",
              "1 0            0             ⋯ 0             0            0             \n",
              "2 1            1             ⋯ 0             0            0             \n",
              "3 0            5             ⋯ 1             0            0             \n",
              "4 0            1             ⋯ 0             0            1             \n",
              "5 4            0             ⋯ 0             0            0             \n",
              "6 2            1             ⋯ 2             0            0             \n",
              "  word_anyone word_debates word_candidates word_military word_including\n",
              "1 1            0           1               0             0             \n",
              "2 0            0           0               0             0             \n",
              "3 0            0           0               0             0             \n",
              "4 1            0           0               0             0             \n",
              "5 0            6           5               0             1             \n",
              "6 0           11           1               0             1             \n",
              "  word_officials word_terrorist\n",
              "1 1              4             \n",
              "2 0              1             \n",
              "3 0              0             \n",
              "4 0              0             \n",
              "5 0              0             \n",
              "6 0              0             "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis using Logistic Regression Model\n",
        "\n",
        "**Problem:**\n",
        "\n",
        "Can we predict whether a news article is real or fake using the most frequent words in the article text?\n",
        "\n",
        "**Dataset:**\n",
        "\n",
        "*   Target: real_fake (binary: 1 = real, 0 = fake)\n",
        "*   Predictors: Word frequency counts for top 50 words (word_hillary, word_president, etc.)\n",
        "*   Other metadata: title, authors, publish_date, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "O_1Jiyecyo2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the first author from the authors column\n",
        "buzzfeed_data$first_author <- sapply(strsplit(as.character(buzzfeed_data$authors), \" \"), function(x) x[1])\n"
      ],
      "metadata": {
        "id": "yKfzcNSA5Kdn"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nrow(buzzfeed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "6bnPbFSK5M2Y",
        "outputId": "9afb6248-faf7-468f-e0e4-aebc1d208a7b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "133"
            ],
            "text/markdown": "133",
            "text/latex": "133",
            "text/plain": [
              "[1] 133"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the first author from the authors column\n",
        "buzzfeed_data$first_author <- sapply(strsplit(as.character(buzzfeed_data$authors), \" \"), function(x) x[1])\n",
        "\n",
        "# Convert categorical variables to factors, handling NAs\n",
        "buzzfeed_data$authors <- as.factor(buzzfeed_data$first_author)\n",
        "buzzfeed_data$publish_date <- as.factor(buzzfeed_data$publish_date)\n",
        "\n",
        "# Create dummy variables using model.matrix\n",
        "dummies <- model.matrix(real_fake ~ authors + publish_date, data = buzzfeed_data)\n",
        "\n",
        "# Remove the intercept column\n",
        "dummies <- dummies[, -1]\n",
        "\n",
        "# Extract the numeric word_ columns\n",
        "word_columns <- buzzfeed_data %>%\n",
        "  select(starts_with(\"word_\"))\n",
        "\n",
        "# Ensure all data frames have the same number of rows before cbind\n",
        "# This will use the rows present in all dataframes\n",
        "common_rows <- intersect(rownames(buzzfeed_data), rownames(as.data.frame(dummies)))\n",
        "buzzfeed_data <- buzzfeed_data[common_rows, ]\n",
        "word_columns <- word_columns[common_rows, ]\n",
        "dummies <- dummies[rownames(buzzfeed_data), ]\n",
        "\n",
        "# Combine all into a new dataframe\n",
        "buzzfeed_encoded <- cbind(real_fake = buzzfeed_data$real_fake, word_columns, dummies)\n",
        "\n",
        "# Check the new dataframe\n",
        "str(buzzfeed_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "762-Yefc2dX8",
        "outputId": "70997b00-53aa-49cf-e6a9-39b86e39ca3a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'data.frame':\t109 obs. of  104 variables:\n",
            " $ real_fake             : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ word_hillary          : int  1 0 3 1 9 2 1 2 0 4 ...\n",
            " $ word_president        : int  1 0 1 0 6 4 2 6 0 1 ...\n",
            " $ word_debate           : int  0 0 1 0 19 24 0 0 0 7 ...\n",
            " $ word_obama            : int  0 9 1 0 1 6 0 0 0 0 ...\n",
            " $ word_police           : int  2 0 0 4 0 0 0 0 0 0 ...\n",
            " $ word_because          : int  0 1 0 0 4 2 0 1 0 2 ...\n",
            " $ word_american         : int  0 1 5 1 0 1 0 2 0 1 ...\n",
            " $ word_presidential     : int  0 0 0 0 3 2 2 3 0 1 ...\n",
            " $ word_before           : int  2 1 0 0 0 3 0 2 0 0 ...\n",
            " $ word_america          : int  2 2 8 1 0 1 0 3 0 2 ...\n",
            " $ word_election         : int  0 0 3 0 0 3 0 3 0 0 ...\n",
            " $ word_republican       : int  0 0 1 0 2 0 0 5 0 1 ...\n",
            " $ word_foundation       : int  0 0 1 0 0 0 0 1 0 2 ...\n",
            " $ word_according        : int  0 0 0 0 0 0 0 1 0 0 ...\n",
            " $ word_every            : int  1 2 6 1 1 5 0 3 0 0 ...\n",
            " $ word_september        : int  2 0 0 2 4 1 0 0 0 0 ...\n",
            " $ word_united           : int  0 7 5 0 2 0 0 0 0 0 ...\n",
            " $ word_another          : int  0 0 1 0 1 0 0 0 0 0 ...\n",
            " $ word_political        : int  2 0 2 0 0 6 0 0 0 0 ...\n",
            " $ word_candidate        : int  1 0 1 0 9 3 3 7 0 1 ...\n",
            " $ word_americans        : int  0 1 1 0 0 1 0 0 0 1 ...\n",
            " $ word_national         : int  2 3 1 0 3 0 0 0 0 0 ...\n",
            " $ word_charlotte        : int  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ word_democratic       : int  0 0 3 0 0 0 0 3 0 0 ...\n",
            " $ word_policy           : int  0 0 0 0 3 0 0 0 0 0 ...\n",
            " $ word_something        : int  0 2 0 0 0 2 0 0 0 0 ...\n",
            " $ word_washington       : int  0 0 0 0 0 1 1 0 2 0 ...\n",
            " $ word_nominee          : int  0 0 0 0 2 0 0 5 0 1 ...\n",
            " $ word_security         : int  0 2 1 0 1 0 0 0 0 0 ...\n",
            " $ word_federal          : int  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ word_terrorism        : int  7 0 0 0 0 0 0 0 0 0 ...\n",
            " $ word_department       : int  0 0 0 0 0 0 0 0 0 1 ...\n",
            " $ word_government       : int  0 3 0 0 2 2 0 0 0 0 ...\n",
            " $ word_actually         : int  0 2 0 0 1 0 2 0 0 0 ...\n",
            " $ word_rahami           : int  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ word_continued        : int  0 0 2 0 1 1 1 0 0 1 ...\n",
            " $ word_everyone         : int  0 1 2 1 1 0 0 0 0 0 ...\n",
            " $ word_general          : int  0 1 1 0 0 0 0 0 0 0 ...\n",
            " $ word_office           : int  0 0 0 2 0 0 0 1 0 0 ...\n",
            " $ word_terrorists       : int  1 0 0 0 0 0 0 0 0 0 ...\n",
            " $ word_business         : int  0 0 1 0 0 2 0 1 0 0 ...\n",
            " $ word_islamic          : int  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ word_statement        : int  0 0 0 1 0 0 0 1 0 0 ...\n",
            " $ word_anyone           : int  1 0 0 1 0 0 0 0 0 0 ...\n",
            " $ word_debates          : int  0 0 0 0 6 11 0 0 0 0 ...\n",
            " $ word_candidates       : int  1 0 0 0 5 1 3 2 0 0 ...\n",
            " $ word_military         : int  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ word_including        : int  0 0 0 0 1 1 0 0 0 0 ...\n",
            " $ word_officials        : int  1 0 0 0 0 0 0 0 0 0 ...\n",
            " $ word_terrorist        : int  4 1 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsBob            : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsBrianna        : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsCampus         : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsCassy          : num  0 1 0 1 0 0 0 0 0 0 ...\n",
            " $ authorsColin          : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsCrispin        : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsEdward-isaac   : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsElvin          : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsFeatured       : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsFed            : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsGrant          : num  0 0 0 0 0 0 0 1 0 0 ...\n",
            " $ authorsHadas          : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsJack           : num  0 0 1 0 1 1 1 0 0 1 ...\n",
            " $ authorsJohn           : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsJosh           : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsKevin          : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsLatest         : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsLisa           : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsMadeline       : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsMartin         : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsMockarena      : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsNick           : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsOliver         : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsOnan           : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsPhilip         : num  0 0 0 0 0 0 0 0 1 0 ...\n",
            " $ authorsRich           : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsRika           : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsRyan           : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsScott          : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsSierra         : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsStephen        : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsSteven         : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsTerresa        : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsTiffiny        : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsView           : num  1 0 0 0 0 0 0 0 0 0 ...\n",
            " $ authorsWendy          : num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ publish_date2015-05-26: num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ publish_date2016-01-30: num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ publish_date2016-03-27: num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ publish_date2016-06-20: num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ publish_date2016-08-10: num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ publish_date2016-09-18: num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ publish_date2016-09-19: num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ publish_date2016-09-20: num  0 0 0 0 0 0 0 0 0 0 ...\n",
            " $ publish_date2016-09-21: num  0 1 0 1 0 0 0 0 0 0 ...\n",
            " $ publish_date2016-09-22: num  1 0 0 0 0 0 0 0 0 0 ...\n",
            " $ publish_date2016-09-23: num  0 0 0 0 0 0 0 1 0 0 ...\n",
            " $ publish_date2016-09-24: num  0 0 0 0 0 0 0 0 1 0 ...\n",
            "  [list output truncated]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit full model\n",
        "model_full <- glm(real_fake ~ ., data = df %>% select(real_fake, starts_with(\"word_\")), family = binomial)\n",
        "summary(model_full)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CioHgHrYx-xG",
        "outputId": "1ac00e49-e6c0-4da0-deca-385a34165488"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message:\n",
            "“glm.fit: algorithm did not converge”\n",
            "Warning message:\n",
            "“glm.fit: fitted probabilities numerically 0 or 1 occurred”\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Call:\n",
              "glm(formula = real_fake ~ ., family = binomial, data = buzzfeed_encoded)\n",
              "\n",
              "Coefficients: (4 not defined because of singularities)\n",
              "                           Estimate Std. Error z value Pr(>|z|)\n",
              "(Intercept)              -3.485e+01  2.758e+06       0        1\n",
              "word_hillary              1.270e+00  8.149e+04       0        1\n",
              "word_president            1.510e+00  1.455e+05       0        1\n",
              "word_debate              -1.111e+00  1.288e+05       0        1\n",
              "word_obama               -3.122e+00  1.559e+05       0        1\n",
              "word_police              -3.092e-01  2.009e+05       0        1\n",
              "word_because             -6.841e+00  2.807e+05       0        1\n",
              "word_american             7.535e+00  4.950e+05       0        1\n",
              "word_presidential        -5.474e+00  3.431e+05       0        1\n",
              "word_before               1.630e+00  2.099e+05       0        1\n",
              "word_america              1.063e+00  1.938e+05       0        1\n",
              "word_election             8.796e-01  3.087e+05       0        1\n",
              "word_republican          -5.526e-01  5.240e+05       0        1\n",
              "word_foundation          -1.482e+00  6.358e+04       0        1\n",
              "word_according           -4.006e+00  4.321e+05       0        1\n",
              "word_every               -9.358e+00  2.753e+05       0        1\n",
              "word_september           -1.118e+01  3.103e+05       0        1\n",
              "word_united              -5.218e+00  1.873e+05       0        1\n",
              "word_another              8.534e+00  2.973e+05       0        1\n",
              "word_political            7.731e+00  2.491e+05       0        1\n",
              "word_candidate            2.374e+00  3.312e+05       0        1\n",
              "word_americans           -7.533e+00  3.997e+05       0        1\n",
              "word_national             1.794e+00  4.144e+05       0        1\n",
              "word_charlotte            7.582e+00  3.744e+05       0        1\n",
              "word_democratic           9.675e-01  3.051e+05       0        1\n",
              "word_policy               1.186e+01  2.546e+05       0        1\n",
              "word_something            1.803e+00  4.279e+05       0        1\n",
              "word_washington          -1.274e+01  4.792e+05       0        1\n",
              "word_nominee             -8.936e-01  4.923e+05       0        1\n",
              "word_security            -7.275e+00  2.491e+05       0        1\n",
              "word_federal              2.261e+01  6.859e+05       0        1\n",
              "word_terrorism            6.851e+00  2.735e+05       0        1\n",
              "word_department           9.253e+00  2.444e+05       0        1\n",
              "word_government          -8.884e+00  4.844e+05       0        1\n",
              "word_actually             1.738e+01  4.270e+05       0        1\n",
              "word_rahami               2.209e+00  2.987e+05       0        1\n",
              "word_continued           -2.971e+01  9.917e+05       0        1\n",
              "word_everyone             1.282e+01  7.943e+05       0        1\n",
              "word_general              1.438e+00  4.393e+05       0        1\n",
              "word_office              -1.760e+01  1.578e+05       0        1\n",
              "word_terrorists           2.218e+01  5.403e+05       0        1\n",
              "word_business             1.040e+01  3.088e+05       0        1\n",
              "word_islamic             -6.136e+00  2.509e+05       0        1\n",
              "word_statement           -8.261e+00  2.856e+05       0        1\n",
              "word_anyone              -3.098e+01  6.149e+05       0        1\n",
              "word_debates              5.051e+00  2.580e+05       0        1\n",
              "word_candidates          -2.282e-02  4.533e+05       0        1\n",
              "word_military            -8.702e+00  2.842e+05       0        1\n",
              "word_including            2.794e+00  6.926e+05       0        1\n",
              "word_officials           -1.151e+01  5.142e+05       0        1\n",
              "word_terrorist           -1.404e+01  3.223e+05       0        1\n",
              "authorsBob                1.555e+02  4.184e+06       0        1\n",
              "authorsBrianna           -3.626e+01  1.521e+06       0        1\n",
              "authorsCampus            -2.517e+01  9.713e+05       0        1\n",
              "authorsCassy             -8.625e+00  1.327e+06       0        1\n",
              "authorsColin             -2.727e+01  9.947e+05       0        1\n",
              "authorsCrispin            7.438e+01  3.061e+06       0        1\n",
              "`authorsEdward-isaac`    -3.386e+01  6.093e+06       0        1\n",
              "authorsElvin              5.933e+01  2.937e+06       0        1\n",
              "authorsFeatured          -4.016e+01  1.418e+06       0        1\n",
              "authorsFed               -7.004e+01  1.628e+06       0        1\n",
              "authorsGrant             -2.022e+01  4.054e+06       0        1\n",
              "authorsHadas             -1.035e+02  1.197e+06       0        1\n",
              "authorsJack              -4.995e+01  1.556e+06       0        1\n",
              "authorsJohn              -1.033e+02  9.454e+05       0        1\n",
              "authorsJosh              -3.705e+02  1.236e+07       0        1\n",
              "authorsKevin             -2.967e+01  9.561e+05       0        1\n",
              "authorsLatest            -2.022e+01  3.032e+06       0        1\n",
              "authorsLisa              -7.356e+01  1.979e+06       0        1\n",
              "authorsMadeline           8.933e+00  2.723e+06       0        1\n",
              "authorsMartin             3.602e+01  2.050e+06       0        1\n",
              "authorsMockarena         -2.903e+01  1.222e+06       0        1\n",
              "authorsNick              -9.492e+00  2.165e+06       0        1\n",
              "authorsOliver            -1.382e+02  1.731e+06       0        1\n",
              "authorsOnan              -7.867e+01  3.298e+06       0        1\n",
              "authorsPhilip            -6.645e+01  1.619e+06       0        1\n",
              "authorsRich               2.282e+01  2.611e+06       0        1\n",
              "authorsRika               7.523e+00  2.579e+06       0        1\n",
              "authorsRyan              -1.010e+01  1.556e+06       0        1\n",
              "authorsScott              6.677e+01  2.985e+06       0        1\n",
              "authorsSierra            -4.183e+01  1.123e+06       0        1\n",
              "authorsStephen           -2.224e+01  4.539e+06       0        1\n",
              "authorsSteven            -1.740e+01  8.432e+06       0        1\n",
              "authorsTerresa           -4.356e+01  1.287e+06       0        1\n",
              "authorsTiffiny           -2.845e+01  1.088e+06       0        1\n",
              "authorsView              -5.124e+01  1.504e+06       0        1\n",
              "authorsWendy              5.783e+00  1.495e+06       0        1\n",
              "`publish_date2015-05-26` -5.887e+01  3.196e+06       0        1\n",
              "`publish_date2016-01-30`  3.702e+01  2.921e+06       0        1\n",
              "`publish_date2016-03-27`         NA         NA      NA       NA\n",
              "`publish_date2016-06-20`  1.527e+02  3.201e+06       0        1\n",
              "`publish_date2016-08-10`         NA         NA      NA       NA\n",
              "`publish_date2016-09-18`  4.591e+01  4.812e+06       0        1\n",
              "`publish_date2016-09-19`  1.325e+02  3.272e+06       0        1\n",
              "`publish_date2016-09-20`  1.317e+02  3.137e+06       0        1\n",
              "`publish_date2016-09-21`  1.008e+02  3.109e+06       0        1\n",
              "`publish_date2016-09-22`  9.083e+01  3.144e+06       0        1\n",
              "`publish_date2016-09-23`  5.346e+01  2.630e+06       0        1\n",
              "`publish_date2016-09-24`  1.002e+02  3.641e+06       0        1\n",
              "`publish_date2016-09-25`  6.551e+01  3.808e+06       0        1\n",
              "`publish_date2016-09-26`  9.342e+01  3.021e+06       0        1\n",
              "`publish_date2016-09-27`  9.896e+01  3.162e+06       0        1\n",
              "`publish_date2016-09-28`         NA         NA      NA       NA\n",
              "`publish_date2017-08-24`         NA         NA      NA       NA\n",
              "\n",
              "(Dispersion parameter for binomial family taken to be 1)\n",
              "\n",
              "    Null deviance: 1.5088e+02  on 108  degrees of freedom\n",
              "Residual deviance: 7.3503e-10  on   9  degrees of freedom\n",
              "AIC: 200\n",
              "\n",
              "Number of Fisher Scoring iterations: 25\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use stepwise selection based on AIC"
      ],
      "metadata": {
        "id": "V3el_-aozUFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable Selection (Best Subset)\n",
        "model_full <- glm(real_fake ~ ., data = buzzFeed_data, family = binomial)\n",
        "\n",
        "# Perform stepwise selection silently\n",
        "suppressMessages({\n",
        "  model_best <- step(model_full, direction = \"both\", trace = 0)\n",
        "})\n",
        "\n",
        "summary(model_best)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "K1O8Gh8jzOMb",
        "outputId": "9e060039-7410-4c9f-fc6e-ec744b766d8c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ERROR",
          "evalue": "Error in eval(mf, parent.frame()): object 'buzzFeed_data' not found\n",
          "traceback": [
            "Error in eval(mf, parent.frame()): object 'buzzFeed_data' not found\nTraceback:\n",
            "1. eval(mf, parent.frame())",
            "2. eval(mf, parent.frame())",
            "3. stats::model.frame(formula = real_fake ~ ., data = buzzFeed_data, \n .     drop.unused.levels = TRUE)",
            "4. model.frame.default(formula = real_fake ~ ., data = buzzFeed_data, \n .     drop.unused.levels = TRUE)",
            "5. is.data.frame(data)",
            "6. .handleSimpleError(function (cnd) \n . {\n .     watcher$capture_plot_and_output()\n .     cnd <- sanitize_call(cnd)\n .     watcher$push(cnd)\n .     switch(on_error, continue = invokeRestart(\"eval_continue\"), \n .         stop = invokeRestart(\"eval_stop\"), error = NULL)\n . }, \"object 'buzzFeed_data' not found\", base::quote(eval(mf, parent.frame())))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Likelihood ratio test\n",
        "anova(model_best, test = \"Chisq\")\n",
        "\n",
        "# # Hosmer-Lemeshow goodness of fit\n",
        "# install.packages(\"ResourceSelection\")\n",
        "# library(ResourceSelection)\n",
        "# hoslem.test(df$real_fake, fitted(model_best))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "nqbMp81Iznf6",
        "outputId": "acc5f61b-59d6-4bab-9905-39f19290c36d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A anova: 15 × 5</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>Df</th><th scope=col>Deviance</th><th scope=col>Resid. Df</th><th scope=col>Resid. Dev</th><th scope=col>Pr(&gt;Chi)</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>NULL</th><td>NA</td><td>          NA</td><td>181</td><td>252.3056</td><td>          NA</td></tr>\n",
              "\t<tr><th scope=row>word_hillary</th><td> 1</td><td>7.963362e+00</td><td>180</td><td>244.3422</td><td>4.773366e-03</td></tr>\n",
              "\t<tr><th scope=row>word_police</th><td> 1</td><td>2.231569e-01</td><td>179</td><td>244.1191</td><td>6.366450e-01</td></tr>\n",
              "\t<tr><th scope=row>word_because</th><td> 1</td><td>1.405605e-02</td><td>178</td><td>244.1050</td><td>9.056254e-01</td></tr>\n",
              "\t<tr><th scope=row>word_foundation</th><td> 1</td><td>2.837639e+00</td><td>177</td><td>241.2674</td><td>9.207942e-02</td></tr>\n",
              "\t<tr><th scope=row>word_every</th><td> 1</td><td>5.458875e+00</td><td>176</td><td>235.8085</td><td>1.946918e-02</td></tr>\n",
              "\t<tr><th scope=row>word_september</th><td> 1</td><td>5.579383e+00</td><td>175</td><td>230.2291</td><td>1.817312e-02</td></tr>\n",
              "\t<tr><th scope=row>word_candidate</th><td> 1</td><td>1.564608e+01</td><td>174</td><td>214.5830</td><td>7.637078e-05</td></tr>\n",
              "\t<tr><th scope=row>word_washington</th><td> 1</td><td>3.466645e+00</td><td>173</td><td>211.1164</td><td>6.261820e-02</td></tr>\n",
              "\t<tr><th scope=row>word_department</th><td> 1</td><td>7.701084e-05</td><td>172</td><td>211.1163</td><td>9.929982e-01</td></tr>\n",
              "\t<tr><th scope=row>word_continued</th><td> 1</td><td>3.286394e+01</td><td>171</td><td>178.2524</td><td>9.883996e-09</td></tr>\n",
              "\t<tr><th scope=row>word_everyone</th><td> 1</td><td>2.668025e+00</td><td>170</td><td>175.5843</td><td>1.023830e-01</td></tr>\n",
              "\t<tr><th scope=row>word_office</th><td> 1</td><td>1.144341e+01</td><td>169</td><td>164.1409</td><td>7.174796e-04</td></tr>\n",
              "\t<tr><th scope=row>word_military</th><td> 1</td><td>7.629634e+00</td><td>168</td><td>156.5113</td><td>5.741694e-03</td></tr>\n",
              "\t<tr><th scope=row>word_officials</th><td> 1</td><td>3.318402e+00</td><td>167</td><td>153.1929</td><td>6.850838e-02</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA anova: 15 × 5\n\n| <!--/--> | Df &lt;int&gt; | Deviance &lt;dbl&gt; | Resid. Df &lt;int&gt; | Resid. Dev &lt;dbl&gt; | Pr(&gt;Chi) &lt;dbl&gt; |\n|---|---|---|---|---|---|\n| NULL | NA |           NA | 181 | 252.3056 |           NA |\n| word_hillary |  1 | 7.963362e+00 | 180 | 244.3422 | 4.773366e-03 |\n| word_police |  1 | 2.231569e-01 | 179 | 244.1191 | 6.366450e-01 |\n| word_because |  1 | 1.405605e-02 | 178 | 244.1050 | 9.056254e-01 |\n| word_foundation |  1 | 2.837639e+00 | 177 | 241.2674 | 9.207942e-02 |\n| word_every |  1 | 5.458875e+00 | 176 | 235.8085 | 1.946918e-02 |\n| word_september |  1 | 5.579383e+00 | 175 | 230.2291 | 1.817312e-02 |\n| word_candidate |  1 | 1.564608e+01 | 174 | 214.5830 | 7.637078e-05 |\n| word_washington |  1 | 3.466645e+00 | 173 | 211.1164 | 6.261820e-02 |\n| word_department |  1 | 7.701084e-05 | 172 | 211.1163 | 9.929982e-01 |\n| word_continued |  1 | 3.286394e+01 | 171 | 178.2524 | 9.883996e-09 |\n| word_everyone |  1 | 2.668025e+00 | 170 | 175.5843 | 1.023830e-01 |\n| word_office |  1 | 1.144341e+01 | 169 | 164.1409 | 7.174796e-04 |\n| word_military |  1 | 7.629634e+00 | 168 | 156.5113 | 5.741694e-03 |\n| word_officials |  1 | 3.318402e+00 | 167 | 153.1929 | 6.850838e-02 |\n\n",
            "text/latex": "A anova: 15 × 5\n\\begin{tabular}{r|lllll}\n  & Df & Deviance & Resid. Df & Resid. Dev & Pr(>Chi)\\\\\n  & <int> & <dbl> & <int> & <dbl> & <dbl>\\\\\n\\hline\n\tNULL & NA &           NA & 181 & 252.3056 &           NA\\\\\n\tword\\_hillary &  1 & 7.963362e+00 & 180 & 244.3422 & 4.773366e-03\\\\\n\tword\\_police &  1 & 2.231569e-01 & 179 & 244.1191 & 6.366450e-01\\\\\n\tword\\_because &  1 & 1.405605e-02 & 178 & 244.1050 & 9.056254e-01\\\\\n\tword\\_foundation &  1 & 2.837639e+00 & 177 & 241.2674 & 9.207942e-02\\\\\n\tword\\_every &  1 & 5.458875e+00 & 176 & 235.8085 & 1.946918e-02\\\\\n\tword\\_september &  1 & 5.579383e+00 & 175 & 230.2291 & 1.817312e-02\\\\\n\tword\\_candidate &  1 & 1.564608e+01 & 174 & 214.5830 & 7.637078e-05\\\\\n\tword\\_washington &  1 & 3.466645e+00 & 173 & 211.1164 & 6.261820e-02\\\\\n\tword\\_department &  1 & 7.701084e-05 & 172 & 211.1163 & 9.929982e-01\\\\\n\tword\\_continued &  1 & 3.286394e+01 & 171 & 178.2524 & 9.883996e-09\\\\\n\tword\\_everyone &  1 & 2.668025e+00 & 170 & 175.5843 & 1.023830e-01\\\\\n\tword\\_office &  1 & 1.144341e+01 & 169 & 164.1409 & 7.174796e-04\\\\\n\tword\\_military &  1 & 7.629634e+00 & 168 & 156.5113 & 5.741694e-03\\\\\n\tword\\_officials &  1 & 3.318402e+00 & 167 & 153.1929 & 6.850838e-02\\\\\n\\end{tabular}\n",
            "text/plain": [
              "                Df Deviance     Resid. Df Resid. Dev Pr(>Chi)    \n",
              "NULL            NA           NA 181       252.3056             NA\n",
              "word_hillary     1 7.963362e+00 180       244.3422   4.773366e-03\n",
              "word_police      1 2.231569e-01 179       244.1191   6.366450e-01\n",
              "word_because     1 1.405605e-02 178       244.1050   9.056254e-01\n",
              "word_foundation  1 2.837639e+00 177       241.2674   9.207942e-02\n",
              "word_every       1 5.458875e+00 176       235.8085   1.946918e-02\n",
              "word_september   1 5.579383e+00 175       230.2291   1.817312e-02\n",
              "word_candidate   1 1.564608e+01 174       214.5830   7.637078e-05\n",
              "word_washington  1 3.466645e+00 173       211.1164   6.261820e-02\n",
              "word_department  1 7.701084e-05 172       211.1163   9.929982e-01\n",
              "word_continued   1 3.286394e+01 171       178.2524   9.883996e-09\n",
              "word_everyone    1 2.668025e+00 170       175.5843   1.023830e-01\n",
              "word_office      1 1.144341e+01 169       164.1409   7.174796e-04\n",
              "word_military    1 7.629634e+00 168       156.5113   5.741694e-03\n",
              "word_officials   1 3.318402e+00 167       153.1929   6.850838e-02"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df <- buzzfeed_data()\n",
        "\n",
        "df$predicted_prob <- predict(model_best, type = \"response\")"
      ],
      "metadata": {
        "id": "F2KUCLDMz-nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "\n",
        "* Opinion | Remember Nayirah, Witness for Kuwait? (Published 1992), www.nytimes.com/1992/01/06/opinion/remember-nayirah-witness-for-kuwait.html. Accessed 6 May 2025.\n",
        "* Shu, Kai, et al. “FakeNewsNet: A Data Repository with News Content, Social Context and Spatialtemporal Information for Studying Fake News on Social Media.” arXiv.Org, 27 Mar. 2019, arxiv.org/abs/1809.01286.\n",
        "* Mahudeswaran, Deepak. “FakeNewsNet.” Kaggle, 2 Nov. 2018, www.kaggle.com/datasets/mdepak/fakenewsnet/data.\n"
      ],
      "metadata": {
        "id": "csXERMUu-Zsw"
      }
    }
  ]
}